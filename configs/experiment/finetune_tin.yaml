# @package _global_

# Fine-tuning configuration for Tiny ImageNet following Charisoudis et al. (2022)
# Protocol: 100 epochs, 1e-3 base LR, 5 warmup epochs, AdamW.
defaults:
  - override /dataset: tiny_imagenet_clf

dataset:
  augmentation:
    use_randaug: true

dataloader:
  batch_size: 128
  num_workers: 8
  persistent_workers: true

optimizer:
  base_lr: 1e-3
  weight_decay: 0.05
  betas: [0.9, 0.999]

train:
  finetune: true
  epochs: 100
  warmup_epochs: 5
  log_interval: 50
  max_steps: null

wandb:
  mode: online

# Path to the pre-trained MAE checkpoint
pretrained_checkpoint: ???
